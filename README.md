# Information-Retrieval-Report

# گزارش نهایی پروژه بازیابی اطلاعات
## خزیدن وب و بازیابی رتبه‌بندی‌شده با TF-IDF

---

## ۱. مقدمه

هدف این پروژه، طراحی و پیاده‌سازی یک زنجیرهٔ کامل بازیابی اطلاعات (Information Retrieval Pipeline) شامل **خزیدن وب، پردازش اسناد و بازیابی رتبه‌بندی‌شده** است. دامنهٔ خزیدن به وب‌سایت انگلیسی دانشگاه فردوسی مشهد به آدرس `https://en.um.ac.ir` محدود شده و هدف نهایی، بازیابی اسناد مرتبط با پرس‌وجوهای نمونهٔ دانشگاهی با استفاده از مدل برداری **TF-IDF** و معیار **Cosine Similarity** می‌باشد.

این پروژه تلاش می‌کند فرآیند کلاسیک بازیابی اطلاعات را به‌صورت عملی پیاده‌سازی کرده و چالش‌های واقعی وب، از جمله نویز متنی، ساختار نامنظم صفحات و ارزیابی نتایج را پوشش دهد.

---

## ۲. معماری خزندهٔ وب

### ۲.۱ طراحی کلی خزنده

خزندهٔ وب با استفاده از کتابخانهٔ **Scrapy** پیاده‌سازی شده است. Scrapy یک چارچوب غیرهمزمان (asynchronous) و رویدادمحور است که امکان خزیدن کارا و مقیاس‌پذیر صفحات وب را فراهم می‌کند. خزنده از یک نقطهٔ شروع مشخص (`start_urls`) آغاز کرده و با دنبال کردن لینک‌های داخلی، صفحات مرتبط را استخراج می‌کند.

برای جلوگیری از خروج از دامنهٔ مجاز، خزنده به‌طور صریح به دامنهٔ `en.um.ac.ir` محدود شده است. این محدودیت باعث می‌شود فقط صفحات مربوط به وب‌سایت هدف پردازش شوند و از خزیدن صفحات خارجی جلوگیری گردد.

---

### ۲.۲ استخراج محتوای متنی

پس از دریافت هر صفحهٔ HTML، محتوای آن با استفاده از کتابخانهٔ **BeautifulSoup** پردازش می‌شود. در این مرحله:

- تگ‌های غیرمفید مانند `script`، `style`، `nav`، `header` و `footer` حذف می‌شوند.
- متن خالص صفحه استخراج می‌شود.
- فاصله‌های اضافی و نویزهای متنی پاک‌سازی می‌گردند.
- عنوان صفحه (`title`) به‌صورت جداگانه ذخیره می‌شود.

این مرحله نقش مهمی در کاهش نویز و افزایش کیفیت داده‌های ورودی سیستم بازیابی دارد.

---

### ۲.۳ ذخیره‌سازی اسناد خزیده‌شده

هر سند خزیده‌شده شامل اطلاعات زیر است:

- آدرس صفحه (`url`)
- عنوان صفحه (`title`)
- متن استخراج‌شده (`text`)

این داده‌ها به‌صورت خط‌به‌خط در قالب **JSONL** ذخیره می‌شوند تا هم پردازش خطی ساده باشد و هم امکان استفاده در مراحل بعدی بازیابی فراهم شود.

---

## ۳. اتصال داده‌های خزیده‌شده به سیستم بازیابی اطلاعات

### ۳.۱ پیش‌پردازش اسناد

پس از پایان مرحلهٔ خزیدن، اسناد متنی وارد مرحلهٔ پیش‌پردازش می‌شوند. این مرحله شامل عملیات زیر است:

- تبدیل تمام حروف به حروف کوچک (Lowercasing)
- توکن‌سازی متن با استفاده از ابزارهای پردازش زبان طبیعی
- حذف کلمات توقف (Stopwords) زبان انگلیسی
- حذف نشانه‌های نگارشی و توکن‌های غیرحرفی

هدف از این مرحله، استانداردسازی متن و کاهش ابعاد فضای ویژگی‌ها است تا مدل برداری عملکرد بهتری داشته باشد.

---

### ۳.۲ مدل‌سازی برداری با TF-IDF

پس از پیش‌پردازش، اسناد به بردارهای عددی با استفاده از مدل **TF-IDF (Term Frequency – Inverse Document Frequency)** تبدیل می‌شوند. این مدل به هر واژه بر اساس دو عامل وزن اختصاص می‌دهد:

- میزان تکرار واژه در سند (TF)
- میزان کمیابی واژه در کل مجموعهٔ اسناد (IDF)

در نتیجه، واژه‌هایی که هم در سند مهم هستند و هم در کل مجموعه نادرترند، وزن بالاتری دریافت می‌کنند.

---

### ۳.۳ بازیابی و رتبه‌بندی اسناد

برای هر پرس‌وجو:

1. پرس‌وجو پیش‌پردازش می‌شود.
2. به بردار TF-IDF تبدیل می‌شود.
3. شباهت کسینوسی بین بردار پرس‌وجو و بردار اسناد محاسبه می‌گردد.
4. اسناد بر اساس مقدار **Cosine Similarity** به‌صورت نزولی رتبه‌بندی می‌شوند.

این فرآیند یک سیستم بازیابی رتبه‌بندی‌شده کلاسیک را پیاده‌سازی می‌کند.

---

## ۴. تحلیل کیفیت نتایج بازیابی

### ۴.۱ معیارهای ارزیابی

برای ارزیابی کیفیت سیستم بازیابی اطلاعات از معیارهای استاندارد زیر استفاده شده است:

- **Precision (دقت)**: نسبت اسناد مرتبط بازیابی‌شده به کل اسناد بازیابی‌شده.
- **Recall (فراخوانی)**: نسبت اسناد مرتبط بازیابی‌شده به کل اسناد مرتبط موجود.
- **F1-Measure**: میانگین هارمونیک Precision و Recall که تعادل بین این دو معیار را نشان می‌دهد.

استفاده از F1-Measure به‌ویژه در بازیابی اطلاعات اهمیت دارد، زیرا این معیار نسبت به میانگین حسابی محافظه‌کارانه‌تر است و در صورت پایین بودن یکی از دو معیار، مقدار کلی را کاهش می‌دهد.

---

### ۴.۲ نتایج تجربی

سیستم با چند پرس‌وجوی نمونهٔ مرتبط با وب‌سایت دانشگاه فردوسی مشهد مورد ارزیابی قرار گرفت. نتایج کلی به‌صورت زیر مشاهده شد:

- Precision نسبتاً بالا، که نشان می‌دهد اسناد بازیابی‌شده غالباً مرتبط هستند.
- Recall متوسط، که بیانگر آن است که همهٔ اسناد مرتبط لزوماً بازیابی نشده‌اند.
- مقدار F1-Measure نشان‌دهندهٔ تعادل قابل قبول بین دقت و فراخوانی است.

این رفتار برای یک سیستم کلاسیک مبتنی بر TF-IDF قابل انتظار است، زیرا این مدل بر تطابق واژگانی تکیه دارد و روابط معنایی عمیق را در نظر نمی‌گیرد.

---

## ۵. جمع‌بندی و پیشنهادات

در این پروژه، یک سیستم کامل بازیابی اطلاعات شامل خزیدن وب، پردازش اسناد و بازیابی رتبه‌بندی‌شده پیاده‌سازی شد. نتایج نشان می‌دهد که مدل TF-IDF با Cosine Similarity همچنان یک مبنای مناسب برای بازیابی اطلاعات متنی است، اما محدودیت‌هایی نیز دارد.

### پیشنهادات برای بهبود آینده:
- استفاده از مدل‌های پیشرفته‌تر مانند **BM25** برای بهبود رتبه‌بندی
- بهره‌گیری از مدل‌های معنایی مانند **Word Embeddings** یا **Dense Retrieval**
- افزودن مکانیزم **Relevance Feedback** برای بهبود نتایج بر اساس بازخورد کاربر
- ارزیابی دقیق‌تر با مجموعهٔ برچسب‌خوردهٔ استاندارد

---

**نتیجه‌گیری نهایی:**  
پروژه حاضر نشان داد که چگونه می‌توان یک سیستم بازیابی اطلاعات عملی را بر روی داده‌های واقعی وب پیاده‌سازی کرد و چالش‌های مهندسی آن را به‌صورت گام‌به‌گام مدیریت نمود.
